{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1edd737-fd75-4c2a-9e53-cead9f7c537d",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Apache Spark's Structured APIs</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f74c0-be01-4c70-9c4d-fe36a2380428",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Key Merits and Beneftis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d91ff8-8ca5-40db-9a23-d76605e46a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Brooke| 20|\n",
      "| Denny| 31|\n",
      "| Jules| 30|\n",
      "|    TD| 35|\n",
      "|Brooke| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando o nosso primeiro DataFrame dentro do Spark.\n",
    "# Vamos aproveitar a situação e fazer algumas operações.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "spark = SparkSession.builder.appName('Chapter3').getOrCreate()\n",
    "\n",
    "# O método createDataFrame demanda uma lista de tuplas para que cada linha seja construída.\n",
    "# Como segundo argumento, uma lista com o nome das colunas pode ser passada.\n",
    "data_df = spark.createDataFrame([('Brooke', 20), ('Denny', 31), ('Jules', 30),\n",
    "                                ('TD', 35), ('Brooke', 25)],['name', 'age'])\n",
    "\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "921361cf-a4c2-4561-9a91-ccea1d0cbadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Denny|    31.0|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupando os dados conforme o nome e extraindo a idade média.\n",
    "\n",
    "# A sintaxe é um pouco diferente da do pandas, o método agg com um outro método de agregação como argumento deve acompahar 'groupBy'.\n",
    "data_df.groupBy('name').agg(avg('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19301f1b-8b57-4a7d-a9f7-420ae506349b",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The DataFrame API</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Os DataFrames do PySpark são como os do pandas. Têm estrutura de tabelas e permitem a nós fazermos operações sobre os seus dados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2525b50-16a9-4aa6-b1b2-6fdf8575b42f",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Spark's Basic Data Types</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Spark possui uma série de tipos de dados que podem ser designados às colunas dos DataFrames.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dda7a3-648b-4bd4-9112-ac86baadfbb8",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <img src='data_types1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d41b5f-7817-4834-acba-be9fbf2d4fc5",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Schemas and Creating DataFrames</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O <em> schema</em> de um DataFrame é a relação do nome de suas colunas com os seus respectivos tipos de dados.\n",
    "        </li>\n",
    "        <li> \n",
    "            O Spark é capaz de inferir o schema de uma tabela, mas isso pode ser demorado e demandar uma alta capacidade de processamento do computador. Por isso, é recomendável que nós façamos essa tarefa.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a75ac1e-3b1d-4e9f-8e3c-8dc2f5258ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos definir o schema do DataFrame que iremos construir.\n",
    "\n",
    "# Do jeito mais formal. O jeito de se construir lembra o das Pipelines do scikit-learn.\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "         StructField('author', StringType(), False),\n",
    "         StructField('title', StringType(), False),\n",
    "        StructField('pages', IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "460106dc-e6db-47a8-a0a9-3fc5a89716d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---+---------+-----+-------------------+---------+-----+-------------------+\n",
      "| Id|    First| Last|                URL|Published| Hits|          Campaigns|\n",
      "+---+---------+-----+-------------------+---------+-----+-------------------+\n",
      "|  1|    Jules|Damji|https://www.oi1.com| 1/4/2016| 4535|[twitter, LinkedIn]|\n",
      "|  2|   Brooke|Wenig|     htps://oi2.com| 5/5/2018| 8908|[twitter, LinkedIn]|\n",
      "|  3|Tathagata|  Das|     htps://oi3.com|5/12/2018|10568|      [twitter, FB]|\n",
      "+---+---------+-----+-------------------+---------+-----+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do jeito mais simples.\n",
    "schema = '''`Id` INT, `First` STRING, `Last` STRING, `URL` STRING, \n",
    "        `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>'''\n",
    "data=[[1, 'Jules', 'Damji', 'https://www.oi1.com', '1/4/2016', 4535, ['twitter', 'LinkedIn']],\n",
    "    [2, 'Brooke', 'Wenig', 'htps://oi2.com', '5/5/2018', 8908, ['twitter', 'LinkedIn']],\n",
    "     [3, 'Tathagata', 'Das', 'htps://oi3.com', '5/12/2018', 10568,['twitter', 'FB']]]\n",
    "\n",
    "# Criando o DataFrame.\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Pelo printSchema, é possível enxergar que as colunas têm o nome e o tipo de dados desejados.\n",
    "blogs_df.printSchema(),blogs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940d31b-68e6-4741-8d1b-0cb2581a57f1",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Columns and Expressions </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Assim como no pandas e no SQL, os DataFrames Spark admitem que o usuário faça expressões com as suas colunas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1985ce64-2c75-48d6-8714-5f40f63e460f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|    First|(Hits > 1000)|\n",
      "+---------+-------------+\n",
      "|    Jules|         true|\n",
      "|   Brooke|         true|\n",
      "|Tathagata|         true|\n",
      "+---------+-------------+\n",
      "\n",
      "+----------+\n",
      "|(Hits / 2)|\n",
      "+----------+\n",
      "|    2267.5|\n",
      "|    4454.0|\n",
      "|    5284.0|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|(Hits / 2)|\n",
      "+----------+\n",
      "|    2267.5|\n",
      "|    4454.0|\n",
      "|    5284.0|\n",
      "+----------+\n",
      "\n",
      "+---+---------+-----+-------------------+---------+-----+-------------------+------------+\n",
      "| Id|    First| Last|                URL|Published| Hits|          Campaigns|   Full Name|\n",
      "+---+---------+-----+-------------------+---------+-----+-------------------+------------+\n",
      "|  1|    Jules|Damji|https://www.oi1.com| 1/4/2016| 4535|[twitter, LinkedIn]|  JulesDamji|\n",
      "|  2|   Brooke|Wenig|     htps://oi2.com| 5/5/2018| 8908|[twitter, LinkedIn]| BrookeWenig|\n",
      "|  3|Tathagata|  Das|     htps://oi3.com|5/12/2018|10568|      [twitter, FB]|TathagataDas|\n",
      "+---+---------+-----+-------------------+---------+-----+-------------------+------------+\n",
      "\n",
      "+---+---------+-----+-------------------+---------+-----+-------------------+\n",
      "| Id|    First| Last|                URL|Published| Hits|          Campaigns|\n",
      "+---+---------+-----+-------------------+---------+-----+-------------------+\n",
      "|  3|Tathagata|  Das|     htps://oi3.com|5/12/2018|10568|      [twitter, FB]|\n",
      "|  2|   Brooke|Wenig|     htps://oi2.com| 5/5/2018| 8908|[twitter, LinkedIn]|\n",
      "|  1|    Jules|Damji|https://www.oi1.com| 1/4/2016| 4535|[twitter, LinkedIn]|\n",
      "+---+---------+-----+-------------------+---------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Os blogueiros têm mais de 1000 hits?\n",
    "blogs_df.select('First',expr('Hits >1000')).show()\n",
    "\n",
    "# Divida 'Hits' por 2;\n",
    "blogs_df.select(expr('Hits /2')).show()\n",
    "\n",
    "# Podemos fazer a mesma operação com o objeto 'col'.\n",
    "blogs_df.select(col('Hits')/2).show()\n",
    "\n",
    "# Uma nova coluna com nome e sobrenome do blogueiro.\n",
    "blogs_df.withColumn('Full Name', concat(expr('First'), expr('Last'))).show()\n",
    "\n",
    "# Reordenando o DF na ordem inversa dos ID's.\n",
    "blogs_df.sort(col('Id').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92470437-789a-44a8-bf9f-8816d9a3ad0c",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Rows </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           Row é um objeto do pyspark que representa uma linha de DataFrame. Pode conter dados de diversas espécies.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58acfecd-373e-4d74-830e-bb6ba96cf4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reynold'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, 'Reynold', 'Xin', 'htps://oi4.com', 255568, '3/2/2015', ['twitter', 'LinkedIn'])\n",
    "\n",
    "# Podemos acessar o valor de uma das colunas da linha por meio de seu íindice.\n",
    "blog_row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b7d5386-f5b4-4d23-b453-555d574b3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---+\n",
      "|  First|     Last| ID|\n",
      "+-------+---------+---+\n",
      "| Felipe|    Veiga|  1|\n",
      "|   Luiz|   Villar|  2|\n",
      "|Eduardo|Rodrigues|  3|\n",
      "+-------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uma coleção de Rows pode dar origem a um DataFrame.\n",
    "rows = [Row('Felipe', 'Veiga', 1),Row('Luiz', 'Villar', 2), Row('Eduardo', 'Rodrigues', 3)]\n",
    "spark.createDataFrame(rows, '`First`STRING, `Last`STRING, `ID`INT').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381cd32c-2fd6-441e-ba3d-65070c0db90e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Common DataFrame Operations</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Com os conceitos de Column e Row apresentados, vamos nos atentar à leitura de fontes de dados prontas. Como o pandas, o Spark possui os métodos necessários para que isso seja feito.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fb7bfba7-581e-4ffc-86ba-b4d01dac47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso não queira definir todo o schema do DF, você pode definir o tamanho da amostra dos dados os quais o Spark \n",
    "# baseará a sua escolha dos tipos de dados.\n",
    "fire_df = spark.read.csv('Fire_Incidents.csv', samplingRatio=1e-1, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54457c84-179c-4b56-9add-a73d82f8a110",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Não apenas conseguimos ler dados, como também é possível criarmos arquivos de diversas extensões a partir de um DataFrame.\n",
    "        </li>\n",
    "        <li> \n",
    "            O Spark, por padrão, converte a tabela para um arquivo Parquet. A vantagem desse formato é que os data types são preservados em seu metadados, tornando desnecessária a definição de um schema caso o documento seja lido futuramente.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7afcc75-56cd-47a2-8125-01818a3d29c5",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Projections and Filters</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A consulta de dados pertencentes a um DataFrame é inicializada a partir de um \"select\". Caso queiramos filtrar os dados com base em um critério, é possível usar os métodos \"where\" e \"filter\".\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2e7e8e14-d5f8-4e07-9e0b-3cd710d723e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 180:>                                                        (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-----------+\n",
      "|Incident Number|Fire Fatalities|Call Number|\n",
      "+---------------+---------------+-----------+\n",
      "|       11050532|              2|  111530113|\n",
      "+---------------+---------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Listando incêndios fatais.\n",
    "few_fire_df =(fire_df\n",
    "              .select('Incident Number', 'Fire Fatalities', 'Call Number')\n",
    "              .where(col('Fire Fatalities')>1) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b7ff69c-9638-4380-935a-9225b96d3d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 193:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+\n",
      "|neighborhood_district|Number of Incidents|\n",
      "+---------------------+-------------------+\n",
      "|         Inner Sunset|              10970|\n",
      "|       Haight Ashbury|               9528|\n",
      "|         Lincoln Park|                800|\n",
      "|            Japantown|               3828|\n",
      "|          North Beach|              11949|\n",
      "|    Lone Mountain/USF|               7869|\n",
      "|     Western Addition|              24058|\n",
      "|       Bernal Heights|              11807|\n",
      "|          Mission Bay|               8921|\n",
      "|         Hayes Valley|              15124|\n",
      "| Financial Distric...|              47324|\n",
      "|            Lakeshore|               9950|\n",
      "| Bayview Hunters P...|              41135|\n",
      "|    Visitacion Valley|               6106|\n",
      "|       Inner Richmond|               8318|\n",
      "|             Nob Hill|              20141|\n",
      "| Oceanview/Merced/...|               8792|\n",
      "|       Outer Richmond|              14941|\n",
      "|      Treasure Island|               3212|\n",
      "|            Chinatown|              14271|\n",
      "+---------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Vamos contar o número de ocorrências por bairro de São Francisco.\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Agrupando os dados de acordo com o bairro e contando o número de 'Incident Number''s distintos.\n",
    "# Em cima disso, escolhi conferir um outro nome à 'Incident Number'.\n",
    "(fire_df\n",
    "        .select('neighborhood_district', 'Incident Number')\n",
    "        .where(col('neighborhood_district').isNotNull())\n",
    "        .groupBy('neighborhood_district')\n",
    "        .agg(countDistinct('Incident Number').alias('Number of Incidents'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7f3fa-a6eb-4c56-bff7-7d7d5e245a44",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Renaming, adding and dropping columns</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Aqui, faremos algumas operações comuns em análise de dados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eedd9637-d181-494d-a9c8-d94dd84e7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Incident no|\n",
      "+-----------+\n",
      "|    8028304|\n",
      "|    8028303|\n",
      "|    8028309|\n",
      "|    8028314|\n",
      "|    8028319|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming\n",
    "\n",
    "# Vamos dar um outro nome à coluna 'Incident Number'.\n",
    "(fire_df.withColumnRenamed('Incident Number', 'Incident no')\n",
    "        .select('Incident no')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6de004be-cee9-4acc-bd89-bbc90070335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Incident Number: string (nullable = true)\n",
      " |-- Exposure Number: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Incident Date: string (nullable = true)\n",
      " |-- Call Number: string (nullable = true)\n",
      " |-- Alarm DtTm: string (nullable = true)\n",
      " |-- Arrival DtTm: string (nullable = true)\n",
      " |-- Close DtTm: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- Station Area: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- Suppression Units: string (nullable = true)\n",
      " |-- Suppression Personnel: string (nullable = true)\n",
      " |-- EMS Units: string (nullable = true)\n",
      " |-- EMS Personnel: string (nullable = true)\n",
      " |-- Other Units: string (nullable = true)\n",
      " |-- Other Personnel: string (nullable = true)\n",
      " |-- First Unit On Scene: string (nullable = true)\n",
      " |-- Estimated Property Loss: string (nullable = true)\n",
      " |-- Estimated Contents Loss: string (nullable = true)\n",
      " |-- Fire Fatalities: string (nullable = true)\n",
      " |-- Fire Injuries: string (nullable = true)\n",
      " |-- Civilian Fatalities: string (nullable = true)\n",
      " |-- Civilian Injuries: string (nullable = true)\n",
      " |-- Number of Alarms: string (nullable = true)\n",
      " |-- Primary Situation: string (nullable = true)\n",
      " |-- Mutual Aid: string (nullable = true)\n",
      " |-- Action Taken Primary: string (nullable = true)\n",
      " |-- Action Taken Secondary: string (nullable = true)\n",
      " |-- Action Taken Other: string (nullable = true)\n",
      " |-- Detector Alerted Occupants: string (nullable = true)\n",
      " |-- Property Use: string (nullable = true)\n",
      " |-- Area of Fire Origin: string (nullable = true)\n",
      " |-- Ignition Cause: string (nullable = true)\n",
      " |-- Ignition Factor Primary: string (nullable = true)\n",
      " |-- Ignition Factor Secondary: string (nullable = true)\n",
      " |-- Heat Source: string (nullable = true)\n",
      " |-- Item First Ignited: string (nullable = true)\n",
      " |-- Human Factors Associated with Ignition: string (nullable = true)\n",
      " |-- Structure Type: string (nullable = true)\n",
      " |-- Structure Status: string (nullable = true)\n",
      " |-- Floor of Fire Origin: string (nullable = true)\n",
      " |-- Fire Spread: string (nullable = true)\n",
      " |-- No Flame Spead: string (nullable = true)\n",
      " |-- Number of floors with minimum damage: string (nullable = true)\n",
      " |-- Number of floors with significant damage: string (nullable = true)\n",
      " |-- Number of floors with heavy damage: string (nullable = true)\n",
      " |-- Number of floors with extreme damage: string (nullable = true)\n",
      " |-- Detectors Present: string (nullable = true)\n",
      " |-- Detector Type: string (nullable = true)\n",
      " |-- Detector Operation: string (nullable = true)\n",
      " |-- Detector Effectiveness: string (nullable = true)\n",
      " |-- Detector Failure Reason: string (nullable = true)\n",
      " |-- Automatic Extinguishing System Present: string (nullable = true)\n",
      " |-- Automatic Extinguishing Sytem Type: string (nullable = true)\n",
      " |-- Automatic Extinguishing Sytem Perfomance: string (nullable = true)\n",
      " |-- Automatic Extinguishing Sytem Failure Reason: string (nullable = true)\n",
      " |-- Number of Sprinkler Heads Operating: string (nullable = true)\n",
      " |-- Supervisor District: string (nullable = true)\n",
      " |-- neighborhood_district: string (nullable = true)\n",
      " |-- point: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a2dbe0b8-c576-4d9a-8d41-eda538d09a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         Alarm DtTm|\n",
      "+-------------------+\n",
      "|2008-04-01T18:06:37|\n",
      "|2008-04-01T18:00:52|\n",
      "+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df.select('Alarm DtTm').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0dc5eb8e-d205-4123-884c-9fe3354d7311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|year(New)|\n",
      "+---------+\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alteração de datatypes.\n",
    "\n",
    "# Vamos usar métodos do pyspark para converter o tipo de uma coluna para timestamp.\n",
    "((fire_df\n",
    "        .withColumn('New', to_timestamp(col('Alarm DtTm'), format='yyyy-MM-ddhh:mm:ss')))\n",
    "        .select(year('New'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6d2c6-f18e-4845-9889-2893f9ff5fd9",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Aggregations</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Usando o groupBy e outras funções estatísticas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d48ada88-7d8f-40db-b001-e8c4455f5e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 248:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|   Primary Situation|Number of Occurences|\n",
      "+--------------------+--------------------+\n",
      "|711 - Municipal a...|               53324|\n",
      "|700 False alarm o...|               30973|\n",
      "|700 - False alarm...|               22254|\n",
      "|745 - Alarm syste...|               19571|\n",
      "|735 - Alarm syste...|               15314|\n",
      "|711 Municipal ala...|               14130|\n",
      "|745 Alarm system ...|               13974|\n",
      "|311 - Medical ass...|               12807|\n",
      "|500 Service Call,...|               12525|\n",
      "|735 Alarm system ...|               12075|\n",
      "|113 - Cooking fir...|               12046|\n",
      "|743 Smoke detecto...|               11215|\n",
      "|740 - Unintention...|               11155|\n",
      "|322 Motor vehicle...|               10862|\n",
      "|322 - Vehicle acc...|               10017|\n",
      "|733 - Smoke detec...|                8683|\n",
      "|500 - Service Cal...|                8624|\n",
      "|118 - Trash or ru...|                8612|\n",
      "|651 - Smoke scare...|                7642|\n",
      "|  554 Assist invalid|                7471|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Quais são as situações primárias mais comuns?\n",
    "(fire_df\n",
    "        .select('Primary Situation')\n",
    "        .where((col('Incident Number').isNotNull())&(col('Primary Situation').isNotNull()))\n",
    "        .groupBy('Primary Situation')\n",
    "        .agg(count('Primary Situation').alias('Number of Occurences'))\n",
    "        .orderBy('Number of Occurences',ascending=False)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b685ea-cdf5-410c-85c5-70650725b288",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Note que é possível coletar múltiplas estatísticas em uma única query.\n",
    "        </li>\n",
    "        <li> \n",
    "            Antes de fazermos a próxima query, observe que muitos dos métodos matemáticos do PySpark têm nomes iguais aos daqueles próprios do Python. Por isso, evite o comando \"from pyspark.sql.functions import *\".\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8edddc91-121f-4388-bdf6-954eb1771c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 258:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+\n",
      "|avg(Suppression Units)|max(Civilian Injuries)|\n",
      "+----------------------+----------------------+\n",
      "|     2.524187052404998|                     9|\n",
      "+----------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Medindo o número médio de unidades de supressão usadas e a quantidade máxima de civis lesionados registrados \n",
    "# em uma única ocorrência.\n",
    "(fire_df\n",
    "        .select(F.avg('Suppression Units'), F.max('Civilian Injuries'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4f2c3-af16-4838-8247-3b2e997073c8",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> The Catalyst Optimizer</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           O Catalyst Optimizer é um dos principais componentes da engine SQL do Spark. Esse é responsável por otimizar os processos de uma query.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6f4b17c5-e3a2-47b0-b778-5b4be5582d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Number of Occurences DESC NULLS LAST], true\n",
      "+- Aggregate [Primary Situation#5780], [Primary Situation#5780, count(Primary Situation#5780) AS Number of Occurences#11908L]\n",
      "   +- Project [Primary Situation#5780]\n",
      "      +- Filter (isnotnull(Incident Number#5752) AND isnotnull(Primary Situation#5780))\n",
      "         +- Project [Primary Situation#5780, Incident Number#5752]\n",
      "            +- Relation [Incident Number#5752,Exposure Number#5753,ID#5754,Address#5755,Incident Date#5756,Call Number#5757,Alarm DtTm#5758,Arrival DtTm#5759,Close DtTm#5760,City#5761,zipcode#5762,Battalion#5763,Station Area#5764,Box#5765,Suppression Units#5766,Suppression Personnel#5767,EMS Units#5768,EMS Personnel#5769,Other Units#5770,Other Personnel#5771,First Unit On Scene#5772,Estimated Property Loss#5773,Estimated Contents Loss#5774,Fire Fatalities#5775,... 40 more fields] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Primary Situation: string, Number of Occurences: bigint\n",
      "Sort [Number of Occurences#11908L DESC NULLS LAST], true\n",
      "+- Aggregate [Primary Situation#5780], [Primary Situation#5780, count(Primary Situation#5780) AS Number of Occurences#11908L]\n",
      "   +- Project [Primary Situation#5780]\n",
      "      +- Filter (isnotnull(Incident Number#5752) AND isnotnull(Primary Situation#5780))\n",
      "         +- Project [Primary Situation#5780, Incident Number#5752]\n",
      "            +- Relation [Incident Number#5752,Exposure Number#5753,ID#5754,Address#5755,Incident Date#5756,Call Number#5757,Alarm DtTm#5758,Arrival DtTm#5759,Close DtTm#5760,City#5761,zipcode#5762,Battalion#5763,Station Area#5764,Box#5765,Suppression Units#5766,Suppression Personnel#5767,EMS Units#5768,EMS Personnel#5769,Other Units#5770,Other Personnel#5771,First Unit On Scene#5772,Estimated Property Loss#5773,Estimated Contents Loss#5774,Fire Fatalities#5775,... 40 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Number of Occurences#11908L DESC NULLS LAST], true\n",
      "+- Aggregate [Primary Situation#5780], [Primary Situation#5780, count(Primary Situation#5780) AS Number of Occurences#11908L]\n",
      "   +- Project [Primary Situation#5780]\n",
      "      +- Filter (isnotnull(Incident Number#5752) AND isnotnull(Primary Situation#5780))\n",
      "         +- Relation [Incident Number#5752,Exposure Number#5753,ID#5754,Address#5755,Incident Date#5756,Call Number#5757,Alarm DtTm#5758,Arrival DtTm#5759,Close DtTm#5760,City#5761,zipcode#5762,Battalion#5763,Station Area#5764,Box#5765,Suppression Units#5766,Suppression Personnel#5767,EMS Units#5768,EMS Personnel#5769,Other Units#5770,Other Personnel#5771,First Unit On Scene#5772,Estimated Property Loss#5773,Estimated Contents Loss#5774,Fire Fatalities#5775,... 40 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Number of Occurences#11908L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(Number of Occurences#11908L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#2477]\n",
      "      +- HashAggregate(keys=[Primary Situation#5780], functions=[count(Primary Situation#5780)], output=[Primary Situation#5780, Number of Occurences#11908L])\n",
      "         +- Exchange hashpartitioning(Primary Situation#5780, 200), ENSURE_REQUIREMENTS, [id=#2474]\n",
      "            +- HashAggregate(keys=[Primary Situation#5780], functions=[partial_count(Primary Situation#5780)], output=[Primary Situation#5780, count#11912L])\n",
      "               +- Project [Primary Situation#5780]\n",
      "                  +- Filter (isnotnull(Incident Number#5752) AND isnotnull(Primary Situation#5780))\n",
      "                     +- FileScan csv [Incident Number#5752,Primary Situation#5780] Batched: false, DataFilters: [isnotnull(Incident Number#5752), isnotnull(Primary Situation#5780)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/veiga/Documents/Python/PySpark/Learning Spark/Chapter 3/Fir..., PartitionFilters: [], PushedFilters: [IsNotNull(Incident Number), IsNotNull(Primary Situation)], ReadSchema: struct<Incident Number:string,Primary Situation:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para visualizar os processos executados em uma query, basta utilizar 'explain(True)'\n",
    "gp = (fire_df\n",
    "        .select('Primary Situation')\n",
    "        .where((col('Incident Number').isNotNull())&(col('Primary Situation').isNotNull()))\n",
    "        .groupBy('Primary Situation')\n",
    "        .agg(count('Primary Situation').alias('Number of Occurences'))\n",
    "        .orderBy('Number of Occurences',ascending=False))\n",
    "\n",
    "gp.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ee1659-93f5-43d6-8886-418e45015ac5",
   "metadata": {},
   "source": [
    "<p style='color:red'>Capítulo 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16733c68-612d-4e91-b2e8-d9a0e67aa3bd",
   "metadata": {},
   "source": [
    "<a href='https://databricks.com/notebooks/gallery/SanFranciscoFireCallsAnalysis.html'> Link do dataset de chamados de incêndio</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
