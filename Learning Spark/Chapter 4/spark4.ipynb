{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0efbed1-f2d7-4348-8e01-5c1c7d197b05",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'>Spark SQL and DataFrames: Introduction to Built-in Data Sources</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5773869-f33d-4465-8e64-acea7a6da47a",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Using Spark SQL in Spark Applications</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Neste capítulo, nós ainda lidaremos com outras formas de manipularmos DataFrames no Spark.\n",
    "        </li>\n",
    "        <li> \n",
    "            Uma funcionalidade que não foi explorada no capítulo anterior é a de se fazer queries SQL nos DF's. Basta criarmos uma View Temporária.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37986289-ecd8-4fe4-8502-1f33ad26dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/01 19:33:08 WARN Utils: Your hostname, veiga-Inspiron resolves to a loopback address: 127.0.1.1; using 192.168.15.21 instead (on interface wlp7s0)\n",
      "22/06/01 19:33:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/veiga/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/01 19:33:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Chapter 4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d34e0395-ab77-4dd8-a056-780499938a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv',\n",
    "           dtype={'date':str, 'delay':np.int32, 'distance':np.int32, 'origin':str, 'destination':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13857a0e-4e83-4607-9d3f-22b3b03e87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.read_csv('https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv',\n",
    "           dtype={'date':str, 'delay':np.int32, 'distance':np.int32, 'origin':str, 'destination':str}).to_csv('departuredelays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4afde762-5bbd-4769-a20b-cfc8b2b63097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iremos trabalhar com um dataset sobre vôos de avião.\n",
    "schema = '`code` STRING, `date` STRING, `delay` INT, `distance` FLOAT, `origin` STRING, `destination` STRING' \n",
    "df = spark.read.csv('departuredelays.csv', schema=schema, header=True)\n",
    "\n",
    "# Montando uma view temporária do DataFrame (é com ela que montaremos as nossas queries).\n",
    "df.createOrReplaceTempView('us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89728cd7-fdd9-4580-b096-9f0db947db26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/02 18:51:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , date, delay, distance, origin, destination\n",
      " Schema: code, date, delay, distance, origin, destination\n",
      "Expected: code but found: \n",
      "CSV file: file:///home/veiga/Documents/Python/PySpark/Learning%20Spark/Chapter%204/departuredelays.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+--------+------+-----------+\n",
      "|  code|    date|delay|distance|origin|destination|\n",
      "+------+--------+-----+--------+------+-----------+\n",
      "|632432|02061625|   -9|  4330.0|   HNL|        JFK|\n",
      "|632091|02021625|   -5|  4330.0|   HNL|        JFK|\n",
      "|632347|02051625|   -8|  4330.0|   HNL|        JFK|\n",
      "|632007|02011625|   -1|  4330.0|   HNL|        JFK|\n",
      "|632614|02081625|   -1|  4330.0|   HNL|        JFK|\n",
      "+------+--------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quais vôos percorreram mais de 1000 milhas?\n",
    "spark.sql('''\n",
    "        SELECT *\n",
    "        FROM us_delay_flights_tbl\n",
    "        WHERE distance > 1000\n",
    "        ORDER BY distance DESC\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33178a95-c498-4798-88da-e07ddf33ec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/02 18:51:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , date, delay, distance, origin, destination\n",
      " Schema: code, date, delay, distance, origin, destination\n",
      "Expected: code but found: \n",
      "CSV file: file:///home/veiga/Documents/Python/PySpark/Learning%20Spark/Chapter%204/departuredelays.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+--------+------+-----------+\n",
      "|   code|    date|delay|distance|origin|destination|\n",
      "+-------+--------+-----+--------+------+-----------+\n",
      "| 844767|02190925| 1638|  1604.0|   SFO|        ORD|\n",
      "| 419836|01031755|  396|  1604.0|   SFO|        ORD|\n",
      "| 413008|01022330|  326|  1604.0|   SFO|        ORD|\n",
      "| 424117|01051205|  320|  1604.0|   SFO|        ORD|\n",
      "| 413489|01190925|  297|  1604.0|   SFO|        ORD|\n",
      "| 854740|02171115|  296|  1604.0|   SFO|        ORD|\n",
      "| 420328|01071040|  279|  1604.0|   SFO|        ORD|\n",
      "| 424118|01051550|  274|  1604.0|   SFO|        ORD|\n",
      "|1346285|03120730|  266|  1604.0|   SFO|        ORD|\n",
      "| 422803|01261104|  258|  1604.0|   SFO|        ORD|\n",
      "| 421509|01161210|  225|  1604.0|   SFO|        ORD|\n",
      "| 844476|02091800|  223|  1604.0|   SFO|        ORD|\n",
      "| 422232|01221040|  215|  1604.0|   SFO|        ORD|\n",
      "|1346286|03121155|  203|  1604.0|   SFO|        ORD|\n",
      "| 851308|02111256|  197|  1604.0|   SFO|        ORD|\n",
      "|1335490|03311405|  196|  1604.0|   SFO|        ORD|\n",
      "| 419777|01031920|  193|  1604.0|   SFO|        ORD|\n",
      "| 413015|01021410|  190|  1604.0|   SFO|        ORD|\n",
      "|1343429|03171215|  189|  1604.0|   SFO|        ORD|\n",
      "| 413246|01101410|  184|  1604.0|   SFO|        ORD|\n",
      "+-------+--------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vôos entre Sâo Francisco e Chicago com atraso de, no mínimo, 2 horas.\n",
    "spark.sql('''\n",
    "    SELECT *\n",
    "    FROM us_delay_flights_tbl\n",
    "    WHERE origin = 'SFO'\n",
    "    AND destination = 'ORD'\n",
    "    AND delay>=2\n",
    "    ORDER BY delay DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a06adcec-c690-4159-8fac-162d8bc2e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descubra as épocas do ano em que os atrasos costumam ser maiores em vôos entre São Francisco e Chicago.\n",
    "new_df = df.withColumn('date', F.to_date('date', 'MMddHHmm'))\n",
    "new_df.createOrReplaceTempView('us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ea179129-cd67-48e4-81cc-74854bd6a7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 160:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|MON|     AVERAGE_DELAY|\n",
      "+---+------------------+\n",
      "|  1|13.622734761120263|\n",
      "|  2|19.186471663619745|\n",
      "|  3|13.661927330173775|\n",
      "+---+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "            SELECT MONTH(T.date) MON, AVG(T.delay) AVERAGE_DELAY\n",
    "\n",
    "            FROM \n",
    "            us_delay_flights_tbl T,\n",
    "\n",
    "            (SELECT AVG(delay) AVERAGE\n",
    "            FROM us_delay_flights_tbl) V\n",
    "\n",
    "            WHERE T.origin='SFO'\n",
    "            AND T.destination='ORD'\n",
    "            \n",
    "\n",
    "            GROUP BY MON\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4d7db922-ffa7-4e2c-81d3-d9350cd6f390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|         a|\n",
      "+----------+\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "|1970-01-31|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        SELECT TO_DATE(date) a\n",
    "        FROM us_delay_flights_tbl\n",
    "        ORDER BY date DESC\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "28cfa2e1-a798-4a9f-8986-0ce26a0fee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+-----------+---------------+\n",
      "|code|      date|origin|destination|   Delay_Status|\n",
      "+----+----------+------+-----------+---------------+\n",
      "|   0|1970-01-01|   ABE|        ATL|Tolerable Delay|\n",
      "|   1|1970-01-02|   ABE|        DTW|          Early|\n",
      "|   2|1970-01-02|   ABE|        ATL|          Early|\n",
      "|   3|1970-01-02|   ABE|        ATL|          Early|\n",
      "|   4|1970-01-03|   ABE|        ATL|          Early|\n",
      "+----+----------+------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/04 20:25:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , date, delay, origin, destination\n",
      " Schema: code, date, delay, origin, destination\n",
      "Expected: code but found: \n",
      "CSV file: file:///home/veiga/Documents/Python/PySpark/Learning%20Spark/Chapter%204/departuredelays.csv\n"
     ]
    }
   ],
   "source": [
    "# Aprimorar a query para postar no LinkedIn. Usar um group by para quantificar os casos de 'Delay_Status'.\n",
    "spark.sql('''\n",
    "        SELECT code, date, origin, destination,\n",
    "        CASE\n",
    "            WHEN delay=0 THEN 'On Time'\n",
    "            WHEN delay>0 AND delay<60 THEN 'Tolerable Delay'\n",
    "            WHEN delay>=60  AND delay <120 THEN 'Short Delay'\n",
    "            WHEN delay>=120 THEN 'Long Delay' \n",
    "            ELSE 'Early'\n",
    "        END Delay_Status\n",
    "        \n",
    "        FROM us_delay_flights_tbl\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf61092-8ff0-4fba-af4f-79ef2d4b2e4a",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> SQL Tables and Views</h2>\n",
    "<h3 style='font-size:30px;font-style:italic'> Managed Versus Unmanaged Tables</h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Spark admite criarmos dois tipos de tabelas, as gerenciáiveis e as não-gerenciáiveis. Com as primeiras, o Spark administra tanto os seus dados, quanto os metadados. Já com as segundas, apenas os metadados são cuidados; no caso de um DELETE, por exemplo, os dados que povoam a tabela, surpreendentemente, seriam preservados. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343fd0a-744c-437e-94a7-0dc0d2868242",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Creating SQL Databases and Tables</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Todas as tabelas são armazenadas, por padrão, na base de dados <em> default</em>. Por outro lado, é possível gerar uma nova database.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e98ac17b-0775-4dbe-b3a5-6a0c26771057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando a DB 'learn_spark_db' e utilizando-a.\n",
    "spark.sql('CREATE DATABASE learn_spark_db')\n",
    "spark.sql('USE learn_spark_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a459c2b-7f4f-4d52-b924-69e50b999c72",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Creating a managed table</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fdfde00b-4c40-48a8-a4e0-1bf1022d3d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = 'code STRING ,date STRING, delay INT, distance FLOAT, origin STRING, destination STRING'\n",
    "flights_df = spark.read.csv('departuredelays.csv', schema=schema)\n",
    "flights_df.write.saveAsTable('managed_us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "519acbbc-20aa-4bb1-915c-9fb322b25915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+--------+------+-----------+\n",
      "|  code|    date|delay|distance|origin|destination|\n",
      "+------+--------+-----+--------+------+-----------+\n",
      "|387821|01301041|   12|   241.0|   RIC|        EWR|\n",
      "|387822|01300630|    0|    87.0|   RIC|        IAD|\n",
      "|387823|01300600|   -5|   558.0|   RIC|        ORD|\n",
      "|387824|01311501|   -1|   315.0|   RIC|        CLE|\n",
      "|387825|01310620|    0|   315.0|   RIC|        CLE|\n",
      "+------+--------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM managed_us_delay_flights_tbl').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f39b20-53ee-47d0-8187-02f45d1e8378",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Creating Views</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como em Bancos de Dados Relacionais, views podem ser criadas. Lembrando, elas não armazenam dados como as tabelas e duram apenas no tempo da sessão.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4d555f40-0fcd-49ec-b94c-95c416c060b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Montando duas views.\n",
    "spark.sql('''\n",
    "    CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    "    SELECT *\n",
    "    FROM managed_us_delay_flights_tbl\n",
    "    WHERE origin='SFO'\n",
    "''')\n",
    "\n",
    "spark.sql('''\n",
    "    CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_global_tmp_view AS\n",
    "    SELECT *\n",
    "    FROM managed_us_delay_flights_tbl\n",
    "    WHERE origin='JFK'\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86bf94-39e8-41e5-a0ae-f0a909023b2f",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            View temporárias globais têm que serem acessadas usando o prefixo global_temp.&lt;nome_view>. Para esse tipo de planilha, o Spark cria uma base de dados temporária.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f6b4c9ff-6792-450d-9986-3bc4736fbb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+--------+------+-----------+\n",
      "|  code|    date|delay|distance|origin|destination|\n",
      "+------+--------+-----+--------+------+-----------+\n",
      "|412960|01011250|   55|  2247.0|   SFO|        JFK|\n",
      "|412961|01012230|    0|  2247.0|   SFO|        JFK|\n",
      "|412962|01010705|   -7|  2247.0|   SFO|        JFK|\n",
      "|412963|01010620|   -3|  2246.0|   SFO|        MIA|\n",
      "|412964|01010915|   -3|   293.0|   SFO|        LAX|\n",
      "+------+--------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('global_temp.us_origin_airport_SFO_global_tmp_view').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f009a4-c8fc-497f-a8a0-6dc55676a12d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Viewing the Metadata</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O atributo \"catalog\" do spark nos ajuda a visualizar os metadados das bases de dados e tabelas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a3bcbde6-4f9a-46f0-9087-0de9c53302fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='code', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O 'listColumns' funciona quase como um DESC do Oracle.\n",
    "spark.catalog.listColumns('managed_us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed85b4-cdc5-4c74-ab2b-b90cd21e4fb6",
   "metadata": {},
   "source": [
    "<p style='color:red'> Caching SQL Tables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
