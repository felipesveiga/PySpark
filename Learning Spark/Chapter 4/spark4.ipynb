{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0efbed1-f2d7-4348-8e01-5c1c7d197b05",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'>Spark SQL and DataFrames: Introduction to Built-in Data Sources</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5773869-f33d-4465-8e64-acea7a6da47a",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Using Spark SQL in Spark Applications</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Neste capítulo, nós ainda lidaremos com outras formas de manipularmos DataFrames no Spark.\n",
    "        </li>\n",
    "        <li> \n",
    "            Uma funcionalidade que não foi explorada no capítulo anterior é a de se fazer queries SQL nos DF's. Basta criarmos uma View Temporária.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37986289-ecd8-4fe4-8502-1f33ad26dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/10 15:01:56 WARN Utils: Your hostname, veiga-Inspiron resolves to a loopback address: 127.0.1.1; using 192.168.15.21 instead (on interface wlp7s0)\n",
      "22/06/10 15:01:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/veiga/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/10 15:01:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Chapter 4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13857a0e-4e83-4607-9d3f-22b3b03e87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.read_csv('https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv',\n",
    "           dtype={'date':str, 'delay':np.int32, 'distance':np.int32, 'origin':str, 'destination':str}).to_csv('departuredelays.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a760b93-fd03-4744-abf5-a69b06626850",
   "metadata": {},
   "source": [
    "<h1 style='color:blue'> INICIALIZAR DAQUI!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afde762-5bbd-4769-a20b-cfc8b2b63097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iremos trabalhar com um dataset sobre vôos de avião.\n",
    "schema = '`code` STRING, `date` STRING, `delay` INT, `distance` FLOAT, `origin` STRING, `destination` STRING' \n",
    "df = spark.read.csv('departuredelays.csv', schema=schema, header=True)\n",
    "\n",
    "# Montando uma view temporária do DataFrame (é com ela que montaremos as nossas queries).\n",
    "df.createOrReplaceTempView('us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89728cd7-fdd9-4580-b096-9f0db947db26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/10 15:03:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , date, delay, distance, origin, destination\n",
      " Schema: code, date, delay, distance, origin, destination\n",
      "Expected: code but found: \n",
      "CSV file: file:///home/veiga/Documents/Python/PySpark/Learning%20Spark/Chapter%204/departuredelays.csv\n",
      "[Stage 0:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+--------+------+-----------+\n",
      "|   code|    date|delay|distance|origin|destination|\n",
      "+-------+--------+-----+--------+------+-----------+\n",
      "|1085920|03061625|   -2|  4330.0|   HNL|        JFK|\n",
      "|1085580|03021625|   14|  4330.0|   HNL|        JFK|\n",
      "|1085835|03051625|   -6|  4330.0|   HNL|        JFK|\n",
      "|1085496|03011625|   -1|  4330.0|   HNL|        JFK|\n",
      "|1086102|03081530|    4|  4330.0|   HNL|        JFK|\n",
      "+-------+--------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Quais vôos percorreram mais de 1000 milhas?\n",
    "spark.sql('''\n",
    "        SELECT *\n",
    "        FROM us_delay_flights_tbl\n",
    "        WHERE distance > 1000\n",
    "        ORDER BY distance DESC\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33178a95-c498-4798-88da-e07ddf33ec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/10 15:03:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , date, delay, distance, origin, destination\n",
      " Schema: code, date, delay, distance, origin, destination\n",
      "Expected: code but found: \n",
      "CSV file: file:///home/veiga/Documents/Python/PySpark/Learning%20Spark/Chapter%204/departuredelays.csv\n",
      "[Stage 1:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+--------+------+-----------+\n",
      "|   code|    date|delay|distance|origin|destination|\n",
      "+-------+--------+-----+--------+------+-----------+\n",
      "| 844767|02190925| 1638|  1604.0|   SFO|        ORD|\n",
      "| 419836|01031755|  396|  1604.0|   SFO|        ORD|\n",
      "| 413008|01022330|  326|  1604.0|   SFO|        ORD|\n",
      "| 424117|01051205|  320|  1604.0|   SFO|        ORD|\n",
      "| 413489|01190925|  297|  1604.0|   SFO|        ORD|\n",
      "| 854740|02171115|  296|  1604.0|   SFO|        ORD|\n",
      "| 420328|01071040|  279|  1604.0|   SFO|        ORD|\n",
      "| 424118|01051550|  274|  1604.0|   SFO|        ORD|\n",
      "|1346285|03120730|  266|  1604.0|   SFO|        ORD|\n",
      "| 422803|01261104|  258|  1604.0|   SFO|        ORD|\n",
      "| 421509|01161210|  225|  1604.0|   SFO|        ORD|\n",
      "| 844476|02091800|  223|  1604.0|   SFO|        ORD|\n",
      "| 422232|01221040|  215|  1604.0|   SFO|        ORD|\n",
      "|1346286|03121155|  203|  1604.0|   SFO|        ORD|\n",
      "| 851308|02111256|  197|  1604.0|   SFO|        ORD|\n",
      "|1335490|03311405|  196|  1604.0|   SFO|        ORD|\n",
      "| 419777|01031920|  193|  1604.0|   SFO|        ORD|\n",
      "| 413015|01021410|  190|  1604.0|   SFO|        ORD|\n",
      "|1343429|03171215|  189|  1604.0|   SFO|        ORD|\n",
      "| 413246|01101410|  184|  1604.0|   SFO|        ORD|\n",
      "+-------+--------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Vôos entre Sâo Francisco e Chicago com atraso de, no mínimo, 2 horas.\n",
    "spark.sql('''\n",
    "    SELECT *\n",
    "    FROM us_delay_flights_tbl\n",
    "    WHERE origin = 'SFO'\n",
    "    AND destination = 'ORD'\n",
    "    AND delay>=2\n",
    "    ORDER BY delay DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06adcec-c690-4159-8fac-162d8bc2e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descubra as épocas do ano em que os atrasos costumam ser maiores em vôos entre São Francisco e Chicago.\n",
    "import pyspark.sql.functions as F\n",
    "new_df = df.withColumn('date', F.to_date('date', 'MMddHHmm'))\n",
    "new_df.createOrReplaceTempView('us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea179129-cd67-48e4-81cc-74854bd6a7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|MON|     AVERAGE_DELAY|\n",
      "+---+------------------+\n",
      "|  1|13.622734761120263|\n",
      "|  2|19.186471663619745|\n",
      "|  3|13.661927330173775|\n",
      "+---+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "            SELECT MONTH(T.date) MON, AVG(T.delay) AVERAGE_DELAY\n",
    "\n",
    "            FROM \n",
    "            us_delay_flights_tbl T,\n",
    "\n",
    "            (SELECT AVG(delay) AVERAGE\n",
    "            FROM us_delay_flights_tbl) V\n",
    "\n",
    "            WHERE T.origin='SFO'\n",
    "            AND T.destination='ORD'\n",
    "            \n",
    "\n",
    "            GROUP BY MON\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7db922-ffa7-4e2c-81d3-d9350cd6f390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|         a|\n",
      "+----------+\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "|1970-03-31|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        SELECT TO_DATE(date) a\n",
    "        FROM us_delay_flights_tbl\n",
    "        ORDER BY date DESC\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28cfa2e1-a798-4a9f-8986-0ce26a0fee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+-----------+---------------+\n",
      "|code|      date|origin|destination|   Delay_Status|\n",
      "+----+----------+------+-----------+---------------+\n",
      "|   0|1970-01-01|   ABE|        ATL|Tolerable Delay|\n",
      "|   1|1970-01-02|   ABE|        DTW|          Early|\n",
      "|   2|1970-01-02|   ABE|        ATL|          Early|\n",
      "|   3|1970-01-02|   ABE|        ATL|          Early|\n",
      "|   4|1970-01-03|   ABE|        ATL|          Early|\n",
      "+----+----------+------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/10 15:04:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , date, delay, origin, destination\n",
      " Schema: code, date, delay, origin, destination\n",
      "Expected: code but found: \n",
      "CSV file: file:///home/veiga/Documents/Python/PySpark/Learning%20Spark/Chapter%204/departuredelays.csv\n"
     ]
    }
   ],
   "source": [
    "# Aprimorar a query para postar no LinkedIn. Usar um group by para quantificar os casos de 'Delay_Status'.\n",
    "spark.sql('''\n",
    "        SELECT code, date, origin, destination,\n",
    "        CASE\n",
    "            WHEN delay=0 THEN 'On Time'\n",
    "            WHEN delay>0 AND delay<60 THEN 'Tolerable Delay'\n",
    "            WHEN delay>=60  AND delay <120 THEN 'Short Delay'\n",
    "            WHEN delay>=120 THEN 'Long Delay' \n",
    "            ELSE 'Early'\n",
    "        END Delay_Status\n",
    "        \n",
    "        FROM us_delay_flights_tbl\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf61092-8ff0-4fba-af4f-79ef2d4b2e4a",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> SQL Tables and Views</h2>\n",
    "<h3 style='font-size:30px;font-style:italic'> Managed Versus Unmanaged Tables</h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Spark admite criarmos dois tipos de tabelas, as gerenciáiveis e as não-gerenciáiveis. Com as primeiras, o Spark administra tanto os seus dados, quanto os metadados. Já com as segundas, apenas os metadados são cuidados; no caso de um DELETE, por exemplo, os dados que povoam a tabela, surpreendentemente, seriam preservados. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343fd0a-744c-437e-94a7-0dc0d2868242",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Creating SQL Databases and Tables</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Todas as tabelas são armazenadas, por padrão, na base de dados <em> default</em>. Por outro lado, é possível gerar uma nova database.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98ac17b-0775-4dbe-b3a5-6a0c26771057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando a DB 'learn_spark_db' e utilizando-a.\n",
    "spark.sql('CREATE DATABASE learn_spark_db')\n",
    "spark.sql('USE learn_spark_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a459c2b-7f4f-4d52-b924-69e50b999c72",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Creating a managed table</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdfde00b-4c40-48a8-a4e0-1bf1022d3d39",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Can not create the managed table('`managed_us_delay_flights_tbl`'). The associated location('file:/home/veiga/Documents/Python/PySpark/Learning%20Spark/Chapter%204/spark-warehouse/learn_spark_db.db/managed_us_delay_flights_tbl') already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_730965/1589347104.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'code STRING ,date STRING, delay INT, distance FLOAT, origin STRING, destination STRING'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mflights_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'departuredelays.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mflights_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'managed_us_delay_flights_tbl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Can not create the managed table('`managed_us_delay_flights_tbl`'). The associated location('file:/home/veiga/Documents/Python/PySpark/Learning%20Spark/Chapter%204/spark-warehouse/learn_spark_db.db/managed_us_delay_flights_tbl') already exists."
     ]
    }
   ],
   "source": [
    "schema = 'code STRING ,date STRING, delay INT, distance FLOAT, origin STRING, destination STRING'\n",
    "flights_df = spark.read.csv('departuredelays.csv', schema=schema)\n",
    "flights_df.write.saveAsTable('managed_us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519acbbc-20aa-4bb1-915c-9fb322b25915",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM managed_us_delay_flights_tbl').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f39b20-53ee-47d0-8187-02f45d1e8378",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Creating Views</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como em Bancos de Dados Relacionais, views podem ser criadas. Lembrando, elas não armazenam dados como as tabelas e duram apenas o tempo da sessão.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d555f40-0fcd-49ec-b94c-95c416c060b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montando duas views.\n",
    "spark.sql('''\n",
    "    CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    "    SELECT *\n",
    "    FROM managed_us_delay_flights_tbl\n",
    "    WHERE origin='SFO'\n",
    "''')\n",
    "\n",
    "spark.sql('''\n",
    "    CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_global_tmp_view AS\n",
    "    SELECT *\n",
    "    FROM managed_us_delay_flights_tbl\n",
    "    WHERE origin='JFK'\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86bf94-39e8-41e5-a0ae-f0a909023b2f",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            View temporárias globais têm que serem acessadas usando o prefixo global_temp.&lt;nome_view>. Para esse tipo de planilha, o Spark cria uma base de dados temporária.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4c9ff-6792-450d-9986-3bc4736fbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table('global_temp.us_origin_airport_SFO_global_tmp_view').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f009a4-c8fc-497f-a8a0-6dc55676a12d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Viewing the Metadata</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O atributo \"catalog\" da SparkSession nos ajuda a visualizar os metadados das bases de dados e tabelas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bcbde6-4f9a-46f0-9087-0de9c53302fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O 'listColumns' funciona quase como um DESC do Oracle.\n",
    "spark.catalog.listColumns('managed_us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabfaca5-e073-4071-9630-cd08e9b13e6e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Parquet</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos aprender a ler arquivos de extensão \"parquet\". A vantagem de serem usados no Spark é que eles já possuem o schema da tabela definido em seus metadados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9da764eb-059b-425a-baca-8d80270ebe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- point_of_interest: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Observe que todas as colunas já vêm com o seu devido data type.\n",
    "spark.read.parquet('train.parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89faec2a-b6e7-4ce9-8e21-170ca9d8f267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando uma view temporária a partir de um parquet no Spark SQL.\n",
    "spark.sql('''\n",
    "            CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "            USING parquet \n",
    "            OPTIONS (\n",
    "            path 'train.parquet')\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed85b4-cdc5-4c74-ab2b-b90cd21e4fb6",
   "metadata": {},
   "source": [
    "<p style='color:red'> Caching SQL Tables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
