{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2abcf3-250d-4b63-aaba-bdbb8f4e8f4d",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> PySpark Tutorial</h1>\n",
    "<h2 style='font-size:30px'>Instalação </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b4243c-eac5-4860-9a4f-7d859d5ad602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 58 kB/s  eta 0:00:011   |█▋                              | 14.5 MB 168 kB/s eta 0:26:26     |██████████████████████▍         | 196.7 MB 362 kB/s eta 0:03:54     |█████████████████████████▎      | 221.9 MB 454 kB/s eta 0:02:11     |█████████████████████████████▉  | 262.7 MB 329 kB/s eta 0:00:57\n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 334 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=34c0c7b07e48c4f04eb7b4babbb5bcf6de26074fd7257fc2a15b7776a9e2ead0\n",
      "  Stored in directory: /home/veiga/.cache/pip/wheels/58/94/83/915c9059e4b038e2d43a6058f307fe1c3e8536e5745f3b23b7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e78f67f-c90a-42c2-8497-6ffb0ffb2d32",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Introdução</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "598ef9b1-d6ca-4a34-a21a-3990e1511d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por efeitos de comparação, vamos carregar um arquivo csv no pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/krishnaik06/Pyspark-With-Python/main/test1.csv')\n",
    "df.to_csv('test1.csv', columns=['Name', 'age', 'Experience', 'Salary'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ba630-f801-4183-9a32-279771268be8",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Iremos agora criar uma sessão no Spark. Para isso, é necessário importar o objeto SparkSession, de pyspark.sql.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<strong style='font-size:16px'>Nota: para se usar o pyspark, é necessário ter o Java instalado em sua máquina! </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b9b8e7-e8ce-444a-99f2-fb181420aec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/24 20:22:38 WARN Utils: Your hostname, veiga-Inspiron resolves to a loopback address: 127.0.1.1; using 192.168.15.21 instead (on interface wlp7s0)\n",
      "22/04/24 20:22:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/veiga/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/24 20:22:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/24 20:22:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.15.21:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc60c0d7130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A seção deve ter um determinado nome.\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate() \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e64943-0520-4ea4-8658-face80d1abc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lendo um arquivo csv. Note como a sintaxe é semelhante com a do pandas.\n",
    "# O pyspark, por padrão, não considera a primeira linha do csv como o header. Para que isso aconteça, defina o argumento\n",
    "# 'header', de 'read.csv' como 'true'.\n",
    "\n",
    "# Veja, os argumentos booleanos aceitam também strings.\n",
    "df_pyspark = spark.read.csv('test1.csv', header='true')\n",
    "\n",
    "# Outra maneira de se fazer uma leitura customizada dos dados.\n",
    "df_pyspark_option = spark.read.option('header', 'true').csv('test1.csv')\n",
    "\n",
    "# Logo de cara, já notamos a estética distinta dos DataFrames do pyspark.\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88f00724-3405-41f5-9dbf-560a50940e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Row(Name='Krish', age=31, Experience=10, Salary=30000),\n",
       "  Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000),\n",
       "  Row(Name='Sunny', age=29, Experience=4, Salary=20000)],\n",
       " ['Name', 'age', 'Experience', 'Salary'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A biblioteca tem inúmeras semelhanças com o pandas.\n",
    "df_pyspark.head(3) ,df_pyspark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59b28d-7370-4b5c-aa33-1829d9e7b388",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> DataFrames</h2>\n",
    "</div>\n",
    "<h3 style='font-size:30px;font-style:italic'>Schema </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Por padrão, todas colunas dos DataFrames pyspark são tidas como strings.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e30f9880-3f2f-40cd-8fb2-bbcd55966a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Criando outra seção no pyspark\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()\n",
    "\n",
    "# Lendo novamete o test1.csv\n",
    "df_pyspark = spark.read.csv('test1.csv', header='true')\n",
    "\n",
    "# Veja, mesmo as colunas que queremos que sejam numéricas estâo como texto.\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c23d95-f863-4796-979d-1f83616b8bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neste caso read.csv tem o argumento inferSchema, capaz de inferir o tipo de dado das colunas.\n",
    "df_pyspark = spark.read.csv('test1.csv', header='true', inferSchema='true')\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fca46d-d34b-41e6-b2f4-6122c324344d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Select</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            A seleção de colunas específicas dos DataFrames é um pouco diferente no pyspark. Para essa operação, é necessário usar o método <em>select </em>.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f08a50f-ff65-4315-bab2-6127e46e1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     Name|Salary|\n",
      "+---------+------+\n",
      "|    Krish| 30000|\n",
      "|Sudhanshu| 25000|\n",
      "|    Sunny| 20000|\n",
      "|     Paul| 20000|\n",
      "|   Harsha| 15000|\n",
      "|  Shubham| 18000|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Coletando apenas o nome do funcionário e seu salário.\n",
    "df_pyspark.select(['Name', 'Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8105d94a-4fba-4b9a-8555-200fb98603e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-----------------+------------------+\n",
      "|summary|  Name|               age|       Experience|            Salary|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "|  count|     6|                 6|                6|                 6|\n",
      "|   mean|  null|26.333333333333332|4.666666666666667|21333.333333333332|\n",
      "| stddev|  null| 4.179314138308661|3.559026084010437| 5354.126134736337|\n",
      "|    min|Harsha|                21|                1|             15000|\n",
      "|    max| Sunny|                31|               10|             30000|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Outro método do pyspark muito similar ao do pandas é 'describe'.\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d1f11-ed23-478b-9ede-5efa6dadb46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4b63cca-d4c8-402e-82ab-c59b4be1f832",
   "metadata": {},
   "source": [
    "<p style='color:red'> Parei em 14:10</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
