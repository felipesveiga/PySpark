{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2abcf3-250d-4b63-aaba-bdbb8f4e8f4d",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> PySpark Tutorial</h1>\n",
    "<h2 style='font-size:30px'>Instalação </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b4243c-eac5-4860-9a4f-7d859d5ad602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/veiga/.local/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /home/veiga/.local/lib/python3.8/site-packages (from pyspark) (0.10.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e78f67f-c90a-42c2-8497-6ffb0ffb2d32",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Introdução</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598ef9b1-d6ca-4a34-a21a-3990e1511d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por efeitos de comparação, vamos carregar um arquivo csv no pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/krishnaik06/Pyspark-With-Python/main/test1.csv')\n",
    "df.to_csv('test1.csv', columns=['Name', 'age', 'Experience', 'Salary'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ba630-f801-4183-9a32-279771268be8",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Iremos agora criar uma sessão no Spark. Para isso, é necessário importar o objeto SparkSession, de pyspark.sql.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<strong style='font-size:16px'>Nota: para se usar o pyspark, é necessário ter o Java instalado em sua máquina! </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b9b8e7-e8ce-444a-99f2-fb181420aec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/30 19:18:10 WARN Utils: Your hostname, veiga-Inspiron resolves to a loopback address: 127.0.1.1; using 192.168.15.21 instead (on interface wlp7s0)\n",
      "22/04/30 19:18:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/veiga/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/30 19:18:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.15.21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7faad03578b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A seção deve ter um determinado nome.\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate() \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e64943-0520-4ea4-8658-face80d1abc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lendo um arquivo csv. Note como a sintaxe é semelhante com a do pandas.\n",
    "# O pyspark, por padrão, não considera a primeira linha do csv como o header. Para que isso aconteça, defina o argumento\n",
    "# 'header', de 'read.csv' como 'true'.\n",
    "\n",
    "# Veja, os argumentos booleanos aceitam também strings.\n",
    "df_pyspark = spark.read.csv('test1.csv', header='true')\n",
    "\n",
    "# Outra maneira de se fazer uma leitura customizada dos dados.\n",
    "df_pyspark_option = spark.read.option('header', 'true').csv('test1.csv')\n",
    "\n",
    "# Logo de cara, já notamos a estética distinta dos DataFrames do pyspark.\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f00724-3405-41f5-9dbf-560a50940e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Row(Name='Krish', age='31', Experience='10', Salary='30000'),\n",
       "  Row(Name='Sudhanshu', age='30', Experience='8', Salary='25000'),\n",
       "  Row(Name='Sunny', age='29', Experience='4', Salary='20000')],\n",
       " ['Name', 'age', 'Experience', 'Salary'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A biblioteca tem inúmeras semelhanças com o pandas.\n",
    "df_pyspark.head(3) ,df_pyspark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59b28d-7370-4b5c-aa33-1829d9e7b388",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> DataFrames</h2>\n",
    "</div>\n",
    "<h3 style='font-size:30px;font-style:italic'>Schema </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Por padrão, todas colunas dos DataFrames pyspark são tidas como strings.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30f9880-3f2f-40cd-8fb2-bbcd55966a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Criando outra seção no pyspark\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()\n",
    "\n",
    "# Lendo novamete o test1.csv\n",
    "df_pyspark = spark.read.csv('test1.csv', header='true')\n",
    "\n",
    "# Veja, mesmo as colunas que queremos que sejam numéricas estâo como texto.\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c23d95-f863-4796-979d-1f83616b8bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neste caso read.csv tem o argumento inferSchema, capaz de inferir o tipo de dado das colunas.\n",
    "df_pyspark = spark.read.csv('test1.csv', header='true', inferSchema='true')\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fca46d-d34b-41e6-b2f4-6122c324344d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Select</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            A seleção de colunas específicas dos DataFrames é um pouco diferente no pyspark. Para essa operação, é necessário usar o método <em>select </em>.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f08a50f-ff65-4315-bab2-6127e46e1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     Name|Salary|\n",
      "+---------+------+\n",
      "|    Krish| 30000|\n",
      "|Sudhanshu| 25000|\n",
      "|    Sunny| 20000|\n",
      "|     Paul| 20000|\n",
      "|   Harsha| 15000|\n",
      "|  Shubham| 18000|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Coletando apenas o nome do funcionário e seu salário.\n",
    "df_pyspark.select(['Name', 'Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8105d94a-4fba-4b9a-8555-200fb98603e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-----------------+------------------+\n",
      "|summary|  Name|               age|       Experience|            Salary|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "|  count|     6|                 6|                6|                 6|\n",
      "|   mean|  null|26.333333333333332|4.666666666666667|21333.333333333332|\n",
      "| stddev|  null| 4.179314138308661|3.559026084010437| 5354.126134736337|\n",
      "|    min|Harsha|                21|                1|             15000|\n",
      "|    max| Sunny|                31|               10|             30000|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outro método do pyspark muito similar ao do pandas é 'describe'.\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db85e85-b826-49ad-979e-8259c284d5ba",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Adicionando e Removendo Colunas</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c8a9a70-c0bf-4d62-bde6-92fba37d74be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+-----------------------+\n",
      "|     Name|age|Experience|Salary|Ages with no Experience|\n",
      "+---------+---+----------+------+-----------------------+\n",
      "|    Krish| 31|        10| 30000|                     21|\n",
      "|Sudhanshu| 30|         8| 25000|                     22|\n",
      "|    Sunny| 29|         4| 20000|                     25|\n",
      "|     Paul| 24|         3| 20000|                     21|\n",
      "|   Harsha| 21|         1| 15000|                     20|\n",
      "|  Shubham| 23|         2| 18000|                     21|\n",
      "+---------+---+----------+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A sintaxe de adição de novas colunas é diferente da do pandas.\n",
    "\n",
    "# 'withColumn' não modifica o DF inplace.\n",
    "df_pyspark.withColumn('Ages with no Experience', df_pyspark['age'] - df_pyspark['Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7631c6-43f6-4ec8-9e2c-938e53e56a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     Name|age|Salary|\n",
      "+---------+---+------+\n",
      "|    Krish| 31| 30000|\n",
      "|Sudhanshu| 30| 25000|\n",
      "|    Sunny| 29| 20000|\n",
      "|     Paul| 24| 20000|\n",
      "|   Harsha| 21| 15000|\n",
      "|  Shubham| 23| 18000|\n",
      "+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Por outro lado, drop é idêntico ao pandas. Novamente, o método não modifica o DF inplace.\n",
    "df_pyspark.drop('Experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a06f781-f64e-464a-98da-3674f350fa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "| New Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomeando colunas (também não é inplace).\n",
    "df_pyspark.withColumnRenamed('Name', 'New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4071be-6f2b-4a97-807b-f579f6f7d48e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Lidando com valores perdidos</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c14041db-3f18-4beb-88ea-c76de8a9d1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.15.21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7faad03578b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5760c89-96ff-40ca-bd47-15105b5722ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+\n",
      "|     Name| age|Experience| Salary|\n",
      "+---------+----+----------+-------+\n",
      "|    Krish|31.0|      10.0|30000.0|\n",
      "|Sudhanshu|30.0|       8.0|25000.0|\n",
      "|    Sunny|29.0|       4.0|20000.0|\n",
      "|     Paul|24.0|       3.0|20000.0|\n",
      "|   Harsha|21.0|       1.0|15000.0|\n",
      "|  Shubham|23.0|       2.0|18000.0|\n",
      "|   Mahesh|null|      null|40000.0|\n",
      "|     null|34.0|      10.0|38000.0|\n",
      "|     null|36.0|      null|   null|\n",
      "+---------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neste novo DataFrame, é possível enxergar valores nulos.\n",
    "df_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True).drop('_c0')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651aaecb-90e5-4408-afd8-a1e780e34448",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Drop</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c47af03-dc96-4e11-b515-3c3aba809ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+\n",
      "|     Name| age|Experience| Salary|\n",
      "+---------+----+----------+-------+\n",
      "|    Krish|31.0|      10.0|30000.0|\n",
      "|Sudhanshu|30.0|       8.0|25000.0|\n",
      "|    Sunny|29.0|       4.0|20000.0|\n",
      "|     Paul|24.0|       3.0|20000.0|\n",
      "|   Harsha|21.0|       1.0|15000.0|\n",
      "|  Shubham|23.0|       2.0|18000.0|\n",
      "|   Mahesh|null|      null|40000.0|\n",
      "|     null|34.0|      10.0|38000.0|\n",
      "+---------+----+----------+-------+\n",
      "\n",
      "+---------+----+----------+-------+\n",
      "|     Name| age|Experience| Salary|\n",
      "+---------+----+----------+-------+\n",
      "|    Krish|31.0|      10.0|30000.0|\n",
      "|Sudhanshu|30.0|       8.0|25000.0|\n",
      "|    Sunny|29.0|       4.0|20000.0|\n",
      "|     Paul|24.0|       3.0|20000.0|\n",
      "|   Harsha|21.0|       1.0|15000.0|\n",
      "|  Shubham|23.0|       2.0|18000.0|\n",
      "|     null|34.0|      10.0|38000.0|\n",
      "+---------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Droppando valores nulos.\n",
    "# O atributo 'na' dos DataFrames do pyspark possui uma série de métodos para lidar com Nan.\n",
    "\n",
    "# Lembrando, os métodos não são inplace.\n",
    "\n",
    "# 'how' pode ser 'any' ou 'all'.\n",
    "# thres define o número mínimo de NaN's que a linha tem que ter para ser excluída.\n",
    "df_pyspark.na.drop(how='any', thresh=2).show()\n",
    "\n",
    "# 'subset' diz ao pyspark considerar apenas os nulos da coluna passada como argumento.\n",
    "df_pyspark.na.drop(how='any', subset='Experience').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9afad6-476f-4669-b18d-4cde27faed46",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Fill</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4680c98-f6b8-4391-9d49-76dfa8db2ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+\n",
      "|     Name| age|Experience| Salary|\n",
      "+---------+----+----------+-------+\n",
      "|    Krish|31.0|      10.0|30000.0|\n",
      "|Sudhanshu|30.0|       8.0|25000.0|\n",
      "|    Sunny|29.0|       4.0|20000.0|\n",
      "|     Paul|24.0|       3.0|20000.0|\n",
      "|   Harsha|21.0|       1.0|15000.0|\n",
      "|  Shubham|23.0|       2.0|18000.0|\n",
      "|   Mahesh|null|      null|40000.0|\n",
      "|     null|34.0|      10.0|38000.0|\n",
      "|     null|36.0|      null|   null|\n",
      "+---------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substituindo os nulos por outra coisa (fill).\n",
    "# Podemos também escolher um subset do DataFrame para fazer a operação.\n",
    "df_pyspark.na.fill('Missing', subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3876a2b-73e2-41e7-b1e7-062bbdc61753",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Impute</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91faa260-f0de-427e-801d-2c155b30c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+------------------+-----------+\n",
      "|     Name| age|Experience| Salary|Experience_imputed|age_imputed|\n",
      "+---------+----+----------+-------+------------------+-----------+\n",
      "|    Krish|31.0|      10.0|30000.0|              10.0|       31.0|\n",
      "|Sudhanshu|30.0|       8.0|25000.0|               8.0|       30.0|\n",
      "|    Sunny|29.0|       4.0|20000.0|               4.0|       29.0|\n",
      "|     Paul|24.0|       3.0|20000.0|               3.0|       24.0|\n",
      "|   Harsha|21.0|       1.0|15000.0|               1.0|       21.0|\n",
      "|  Shubham|23.0|       2.0|18000.0|               2.0|       23.0|\n",
      "|   Mahesh|null|      null|40000.0|               4.0|       29.0|\n",
      "|     null|34.0|      10.0|38000.0|              10.0|       34.0|\n",
      "|     null|36.0|      null|   null|               4.0|       36.0|\n",
      "+---------+----+----------+-------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Equivalente ao imputer do sklearn.\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(strategy='median', inputCols=['Experience', 'age'],\n",
    "                 outputCols=[f'{col}_imputed' for col in ['Experience', 'age']])\n",
    "\n",
    "# Mas, estranhamente, as colunas com valores nulos são mantidas, enquanto são criadas outras com os valores corrigidos.\n",
    "# O nome das novas colunas é definido pelo argumento outputCols.\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030a1f7-a945-41b3-87e8-576e6f875d94",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Filter Operation </h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            O comando filter é equivalente à cláusula WHERE do SQL. É com ele que podemos fazer filtragens no DataFrame.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe6a8801-9a56-432b-9953-5fa6dc337436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "+---------+---+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Pracice').getOrCreate()\n",
    "df_pyspark=spark.read.csv('test1.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Nota: podemos definir o número de linhas a serem printadas com o argumento 'n' de 'show'.\n",
    "df_pyspark.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a1dc946-9fc3-4104-ab17-8d9f5ca61e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|Experience|age|\n",
      "+----------+---+\n",
      "|        10| 31|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Observe, a estrututa é muito similar à utilizada no SQL.\n",
    "(df_pyspark.select(['Experience', 'age'])\n",
    "            .filter((df_pyspark['Salary'] >=25000) & (df_pyspark['age'] >30))\n",
    "            .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49437fb1-d3a5-44a9-b523-0667674dc0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# O pyspark também suporta o uso do ~ como negação.\n",
    "df_pyspark.filter(~(df_pyspark['age'] > 25)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097061f2-9ee6-4251-84b1-e455fb95105d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Groupby and Aggregation Function </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf95a453-024f-419c-8aba-f07f1d714005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------+\n",
      "|  Name| Departments|salary|\n",
      "+------+------------+------+\n",
      "| Krish|Data Science| 10000|\n",
      "| Krish|         IOT|  5000|\n",
      "|Mahesh|    Big Data|  4000|\n",
      "| Krish|    Big Data|  4000|\n",
      "|Mahesh|Data Science|  3000|\n",
      "+------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()\n",
    "df_pyspark=spark.read.csv('test3.csv', header=True, inferSchema=True).drop('_c0')\n",
    "df_pyspark.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefe0d2-089e-4332-8197-a0b511592931",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>GroupBy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4cb3987-1557-43b6-9ea8-7ed23ea4e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A sintaxe é praticamente igual à do pandas.\n",
    "df_pyspark.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6900c3-0225-4034-8974-9ac884f43266",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Agg</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            agg basicamente aplica uma função a uma coluna por inteiro.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7aad47c-51f7-40f8-8877-c9bafc777470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtendo a soma dos salários.\n",
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40668ba-19e6-48ee-8c35-f3bfd723325d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> ML I</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Vamos entender um pouco do uso do Pyspark em Machine Learning.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564b9a40-5d16-4f8c-a748-97ef2607df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 19:40:54 WARN Utils: Your hostname, veiga-Inspiron resolves to a loopback address: 127.0.1.1; using 192.168.15.21 instead (on interface wlp7s0)\n",
      "22/05/02 19:40:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/veiga/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/02 19:40:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.15.21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f273a30f8e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ML').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24003609-bbe7-458d-901a-672748c54577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "+---------+---+----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Com este DF, criaremos um modelo de Regressão simples de previsão de salários.\n",
    "training = spark.read.csv('test1.csv', header=True, inferSchema=True)\n",
    "training.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e605322-13e9-446a-a55c-c5e7051cd116",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            A biblioteca tem uma dinâmica um pouco distinta no pré-processamento de dados. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d89cfe4-5ac0-4f91-b29d-7db5566589b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As variáveis dependentes devem ser agrupadas em uma única coluna com o uso do objeto VectorAssembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=['age', 'Experience'], outputCol='Independent Variables')\n",
    "output=featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8fa6e35-dc1b-440c-a892-f8d8cc6077f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+---------------------+\n",
      "|     Name|age|Experience|Salary|Independent Variables|\n",
      "+---------+---+----------+------+---------------------+\n",
      "|    Krish| 31|        10| 30000|          [31.0,10.0]|\n",
      "|Sudhanshu| 30|         8| 25000|           [30.0,8.0]|\n",
      "|    Sunny| 29|         4| 20000|           [29.0,4.0]|\n",
      "+---------+---+----------+------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# O output da transformação feita é uma coluna em que cada linha há uma lista com as variáveis independentes.\n",
    "output.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3780e019-d336-4e9a-a1c2-fb380f80d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+\n",
      "|Independent Variables|Salary|\n",
      "+---------------------+------+\n",
      "|          [31.0,10.0]| 30000|\n",
      "|           [30.0,8.0]| 25000|\n",
      "|           [29.0,4.0]| 20000|\n",
      "+---------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ao final, basta criar um novo DataFrame, cujas colunas sejam as com as variáveis independentes e os valores-alvo.\n",
    "finalized_data = output.select(['Independent Variables', 'Salary'])\n",
    "finalized_data.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2aa6a34-b93c-4803-b684-18343a20b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quebrando o dataset em conjuntos de treino e teste.\n",
    "train_data, test_data = finalized_data.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b79f5bb-0413-4846-8a5c-6fd6c7839d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 19:59:15 WARN Instrumentation: [de309da4] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Ao definir o algoritmo, temos que passar a coluna de features e a de target value como argumentos.\n",
    "regressor = LinearRegression(featuresCol='Independent Variables', labelCol='Salary')\n",
    "\n",
    "# Temos que redefinir 'regressor' como 'regressor.fit()'\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66420c83-4300-492a-bafa-3b044ddcdfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-12.495194156093755,1014.994232987314] 17254.9019607843\n"
     ]
    }
   ],
   "source": [
    "# Coeficiente e ponto de interceptação.\n",
    "print(regressor.coefficients, regressor.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99d52397-3648-4f97-b476-a1f2854844a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2227.2564118627806"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eis aqui o a raiz quadrada do mean squared error das previsões do algoritmo.\n",
    "from numpy import sqrt\n",
    "sqrt(regressor.evaluate(test_data).meanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db1895b2-b987-4925-aaf1-bab2bd2ca1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+------------------+\n",
      "|Independent Variables|Salary|        prediction|\n",
      "+---------------------+------+------------------+\n",
      "|           [21.0,1.0]| 15000|18007.497116493647|\n",
      "|           [23.0,2.0]| 18000| 18997.50096116877|\n",
      "|           [29.0,4.0]| 20000| 20952.51826220684|\n",
      "|          [31.0,10.0]| 30000|27017.493271818534|\n",
      "+---------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimativas.\n",
    "regressor.evaluate(test_data).predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ee840-7218-482f-9e69-89ee257ec17a",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> ML II</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b63cca-d4c8-402e-82ab-c59b4be1f832",
   "metadata": {},
   "source": [
    "<p style='color:red'> Parei em 1:02:40 (começar ML)</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
