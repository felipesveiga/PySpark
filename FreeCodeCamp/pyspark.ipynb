{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2abcf3-250d-4b63-aaba-bdbb8f4e8f4d",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> PySpark Tutorial</h1>\n",
    "<h2 style='font-size:30px'>Instalação </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b4243c-eac5-4860-9a4f-7d859d5ad602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 58 kB/s  eta 0:00:011   |█▋                              | 14.5 MB 168 kB/s eta 0:26:26     |██████████████████████▍         | 196.7 MB 362 kB/s eta 0:03:54     |█████████████████████████▎      | 221.9 MB 454 kB/s eta 0:02:11     |█████████████████████████████▉  | 262.7 MB 329 kB/s eta 0:00:57\n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 334 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=34c0c7b07e48c4f04eb7b4babbb5bcf6de26074fd7257fc2a15b7776a9e2ead0\n",
      "  Stored in directory: /home/veiga/.cache/pip/wheels/58/94/83/915c9059e4b038e2d43a6058f307fe1c3e8536e5745f3b23b7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e78f67f-c90a-42c2-8497-6ffb0ffb2d32",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Introdução</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "598ef9b1-d6ca-4a34-a21a-3990e1511d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por efeitos de comparação, vamos carregar um arquivo csv no pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/krishnaik06/Pyspark-With-Python/main/test1.csv')\n",
    "df.to_csv('test1.csv', columns=['Name', 'age', 'Experience', 'Salary'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ba630-f801-4183-9a32-279771268be8",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Iremos agora criar uma sessão no Spark. Para isso, é necessário importar o objeto SparkSession, de pyspark.sql.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<strong style='font-size:16px'>Nota: para se usar o pyspark, é necessário ter o Java instalado em sua máquina! </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3b9b8e7-e8ce-444a-99f2-fb181420aec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.15.21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd08934bcd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A seção deve ter um determinado nome.\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate() \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5e64943-0520-4ea4-8658-face80d1abc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lendo um arquivo csv. Note como a sintaxe é semelhante com a do pandas.\n",
    "# O pyspark, por padrão, não considera a primeira linha do csv como o header. Para que isso aconteça, defina o argumento\n",
    "# 'header', de 'read.csv' como 'true'.\n",
    "df_pyspark = spark.read.csv('test1.csv', header='true')\n",
    "\n",
    "# Logo de cara, já notamos a estética distinta dos DataFrames do pyspark.\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88f00724-3405-41f5-9dbf-560a50940e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', age='31', Experience='10', Salary='30000'),\n",
       " Row(Name='Sudhanshu', age='30', Experience='8', Salary='25000'),\n",
       " Row(Name='Sunny', age='29', Experience='4', Salary='20000')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A biblioteca tem inúmeras semelhanças com o pandas.\n",
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d1f11-ed23-478b-9ede-5efa6dadb46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4b63cca-d4c8-402e-82ab-c59b4be1f832",
   "metadata": {},
   "source": [
    "<p style='color:red'> Parei em 14:10</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
